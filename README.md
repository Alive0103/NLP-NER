# 智慧典藏——面向动态互联网环境的持续记忆系统（NLP-NER）

## 背景介绍

经典的学习系统往往被部署在封闭环境中，利用预收集的数据集对固定类别的数据进行建模。然而在开放互联网环境中，这种假设难以满足，例如电商平台每天都会新增多种产品，社交媒体上的新热点话题层出不穷。因为新的类别会随时间不断增长，模型需要在数据流中持续地学习新类，持续实体识别中面临的灾难性遗忘问题亟待解决。

命名实体识别（NER）是自然语言处理（NLP）中的一项关键任务，旨在从给定文本中提取并分类特定类型的实体，如人名（Person）、地名（Location）、组织名（Organization）等。传统的NER模型通常在具有预定义实体类别的大规模数据集上进行训练，然后部署在测试数据上进行实体识别。然而，随着应用需求的变化和扩展，在实践中，NER模型在训练过程中没有看到的新实体类的数据不断到达，当只使用新实体类的注释进行微调时，模型在旧类上的性能很容易下降，即灾难性遗忘。因此希望NER模型可以随着时间的推移随着这些新类数据的知识而增量更新以应对隐私问题、内存限制以及重新注释数据的高昂成本等挑战。

### 连续数据流

传统模型假设数据分布是固定或平稳的，训练样本是独立同分布的，所以模型可以一遍又一遍地看到所有任务相同的数据。但当数据变为连续的数据流时，训练数据的分布就是非平稳的，模型从非平稳的数据分布中持续不断地获取知识时，新知识会干扰旧知识，从而导致模型性能的快速下降 。

### 模型性能

静态Al模型不足以应对复杂多变的真实世界环境。如今的Al系统越来越需要释放机器智能体的终身学习能力。但在学习过程中若将知识全部保留会消耗资源且使模型性能大幅下降。

### 神经网络缺陷

终身学习是参考人类的学习方式，使机器通过保存和积累过去所学的知识并用于未来的学习中。但神经网络模型在适应新任务之后，几乎完全忘记之前学习过的任务。

### 现实意义

如LLM微调和训练中的灾难性遗忘问题，以及深度学习的样本遗忘等。

完善人工智能，需要缓解模型在学习新任务过程中的灾难性遗忘问题，即当连续学习的任务越多时，学习下一个任务的速度就越快。

### 发展现状

1. **传统NER模型**：现有的NER模型通常在具有预定义实体类别的大规模数据集上进行训练，然后在测试数据上进行实体识别。这种方法在实体类别固定且数据丰富的情况下表现良好（Li et al., 2020; Wang et al., 2022; Liu et al., 2021; Ma et al., 2022a）。

2. **增量学习的需求**：在实际应用中，新到达的数据可能包含新的实体类别，用户所需的实体类别集也可能不断扩展。因此，NER模型需要进行增量更新以识别新的实体类别。然而，由于隐私问题或内存限制，旧实体类别的训练数据可能不可用（Li and Hoiem, 2017; Zhang et al, 2020）。此外，重新注释所有旧的实体类别既昂贵又耗时（Delange et al., 2021; Bang et al., 2021）。

3. **类增量NER范式**：为了应对这些问题，Monaikul等人（2021）提出使用仅覆盖新实体类别的新数据集进行增量更新模型，成为类增量NER的标准范式。

4. **持续学习方法**：为了降低不断更新NER模型的成本，研究者们提出了利用知识蒸馏（Knowledge Distillation, KD）框架，通过保留旧实体类别的知识来逐步学习新实体类别（Hinton et al., 2015; Monaikul et al., 2021）。这种方法可以在不重新训练整个模型的情况下，逐步引入新的实体类型。

### 存在的问题

1. **未标记实体问题**：在类增量NER中，仅注释新类别意味着其他类别的实体在数据集中被视为“非实体”（“O”）。这导致模型在旧类别上的性能下降，出现灾难性遗忘问题（Lopez-Paz and Ranzato, 2017; Castro et al., 2018）。此外，潜在的未来可能需要的实体类别未被标记，影响模型的学习能力。

2. **混淆问题**：在增量学习过程中，大多数预测错误来自实体和“O”的混淆。模型被错误标记为“O”导致旧实体类别的识别能力下降，且模型学习新类别的能力也随之下降。

3. **不现实的实验设置**：Monaikul等人（2021）在实验中只引入一个或两个实体类别，这与现实世界中每个步骤引入一组新类别的情况不符。此外，持续学习步骤在现实应用中可能是不断增加的，而不是固定的有限步骤。

4. **高昂的重新注释成本**：为了避免旧知识的遗忘，朴素的解决方案是为旧类型和新类型标注数据集并重新训练模型。然而，这种方法计算效率低，且需要大量的人力。

5. **类型间混淆**：新的训练集如果包含很少与旧实体类型相关的信息，旧类型的知识难以保留。特别是当新旧实体类型很少同时出现时，模型容易混淆不同类型的实体。

6. **隐私与存储问题**：当存储限制或安全问题限制了对数据的访问时，为新实体类型重新注释原始训练数据的成本可能很高，甚至不可能，并且随着类型数量的增加，为所有实体注释新数据集变得不切实际且容易出错。

## 核心技术

1. 知识蒸馏

通过将一个大型、复杂的教师模型的知识转移到一个小型、简单的学生模型中，以减少模型复杂性并提高泛化能力。知识蒸馏可以在学习新任务时，通过保留先前任务的知识，减轻先前任务的遗忘，并且可以在学习新任务时提供额外的信息和指导。

   - **方法**：利用现有的NER模型作为教师模型，通过新的训练材料学习新的实体类型，并通过模仿教师在新训练集上的输出，保持旧实体的知识（Hinton等人，2015; Monaikul等人，2021）。
     
   - **优点**：无需重新注释所有旧实体类的数据，能够在一定程度上保留旧知识。
     
   - **缺点**：如果新的训练集中旧实体类型信息较少，旧知识的保留效果会降低，并且可能导致新旧实体类型间的混淆（Masana等人，2020）。而且需要在新的训练数据集中有大量的数据用于蒸馏。这样的假设通常是不现实的，因为NER训练所需的标记级注释非常耗费人力，而且很少，特别是对于新的不可见的类。

2. 学习与复习（Learn-and-Review, L&R）

   - **方法**：在知识蒸馏的基础上，引入复习阶段，通过生成包含旧实体类型的合成样本增强当前训练集，并在增强数据的基础上提取新旧知识，得到一个增强的学生模型。

   - **优点**：减小新旧任务之间的差距，增强模型区分不同类型实体的能力，减少类型间的混淆，提高每一步的性能。

   - **缺点**：生成合成样本可能增加计算复杂度和训练时间（L&R）。

3. 原型学习/实体感知对比学习

   - **方法**：自适应地从“O”类中检测实体聚类，并学习这些实体聚类的判别表示，通过高精度重新标记旧类中的实体，保持旧类性能，同时提升从“O”中分离新类的能力。

   - **优点**：有效解决了类增量NER中的“未标记实体问题”，提高了模型的整体性能。

   - **缺点**：实现复杂度较高，需要精确的实体聚类算法支持（Monaikul等人，2021）。

4. 对抗性匹配（FGM）

对抗性训练通过引入对抗性扰动来增强模型的鲁棒性和泛化能力，可以使模型更加稳健，抵抗对新任务的干扰，同时可以通过在训练过程中引入对抗性样本，促使模型学习到更加通用的特征表示，从而减轻灾难性遗忘的影响。


## 创新点

综合模型的特性，基于NER的少样本类增量学习这个更现实的设置，我们提出了更具有广泛性的模型：

- 使用特定uie模型（uie-base-Pytorch）预训练，直接用于抽取式任务。可以支持多任务学习，通过添加任务类型嵌入，使模型能够在处理不同类型的任务时进行区分。可以提升模型的泛化能力。
  
- 合成数据更加灵活，同时运用伪标签进行半监督学习，提高学习效果
  
- 进行FGM对抗训练，泛化模型
  
- 运用实体感知的对比学习方法和重新标记策略来更好地学习未标记实体和“O”
  
- 为类别增量NER提供了一个更现实和更具挑战性的基准(我们的F1结果优于few-shot相关论文的SOTA成果)

## 研究方法

- 类型增量设置
  - 数据集名字+描述
- UIE模型与预训练
  - BERT 编码器输出
  - 线性层计算起始和结束位置概率
  - 通过 Sigmoid 函数计算起始和结束位置概率
- 原型学习
- LSTM数据合成，用训练好的模型进行标记，作为伪标签
- FGM对抗训练（在原始输入样本ⅹ上加上一个扰动△x得到对抗样本，再用其进行训练）不是用来使合成数据更加真实，而是用于提升模型鲁棒性和泛化能力。


## 使用方法

可使用中文数据集或CONLL2003数据集，进行数据处理后运行train可看到相关效果。

## 成果说明

原始数据为按照原论文（Few-Shot Class-Incremental Learning for Named Entity Recognition）方法进行训练的F1，而后加入不同方法，有所提升。

![效果](https://github.com/Alive0103/NLP-NER/blob/main/img/7.png)

## 更多

相关论文细节正在撰写，会同步更新，详情可移步的[我的博客](https://alive0103.github.io/)

觉得有用的话点个Star不迷路~

![img](https://github.com/Alive0103/XDU-CS-lab/blob/main/img/%E8%A1%A8%E6%83%851.jpg)
